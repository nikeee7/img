{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import send_example_telemetry\n",
    "\n",
    "send_example_telemetry(\"image_classification_notebook\", framework=\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ffedo\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer, AutoImageProcessor\n",
    "from datasets import Dataset, Image\n",
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unified_class</th>\n",
       "      <th>class_id</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Оленевые</td>\n",
       "      <td>5</td>\n",
       "      <td>3cf4207b958eade893a2f1618cf062b8.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Кошки</td>\n",
       "      <td>2</td>\n",
       "      <td>37698901280c871f426d40afe5c373cd.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Заяц</td>\n",
       "      <td>0</td>\n",
       "      <td>20e7b30026001cbfe0b5c0ee16c9ff56.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Кошки</td>\n",
       "      <td>2</td>\n",
       "      <td>a1bc8ea546206ee8fc0f1836fda9a5c1.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Оленевые</td>\n",
       "      <td>5</td>\n",
       "      <td>54eb76914b84db8a0d56f98125abf588.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28010</th>\n",
       "      <td>Оленевые</td>\n",
       "      <td>5</td>\n",
       "      <td>07b420b4fe265b4ed918b46435c025d7.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28011</th>\n",
       "      <td>Пантеры</td>\n",
       "      <td>6</td>\n",
       "      <td>2d1c5918357bbdd729bf79085e55d35e.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28012</th>\n",
       "      <td>Заяц</td>\n",
       "      <td>0</td>\n",
       "      <td>1531efa9f8687e390adf780355acd606.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28013</th>\n",
       "      <td>Кабан</td>\n",
       "      <td>1</td>\n",
       "      <td>2b15eaef0ce9b57b6570709f95a4bea4.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28014</th>\n",
       "      <td>Пантеры</td>\n",
       "      <td>6</td>\n",
       "      <td>d1fec8a6b6be63534c37f0a26e94c7e8.JPG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28015 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      unified_class  class_id                            image_name\n",
       "0          Оленевые         5  3cf4207b958eade893a2f1618cf062b8.JPG\n",
       "1             Кошки         2  37698901280c871f426d40afe5c373cd.JPG\n",
       "2              Заяц         0  20e7b30026001cbfe0b5c0ee16c9ff56.JPG\n",
       "3             Кошки         2  a1bc8ea546206ee8fc0f1836fda9a5c1.JPG\n",
       "4          Оленевые         5  54eb76914b84db8a0d56f98125abf588.JPG\n",
       "...             ...       ...                                   ...\n",
       "28010      Оленевые         5  07b420b4fe265b4ed918b46435c025d7.JPG\n",
       "28011       Пантеры         6  2d1c5918357bbdd729bf79085e55d35e.JPG\n",
       "28012          Заяц         0  1531efa9f8687e390adf780355acd606.JPG\n",
       "28013         Кабан         1  2b15eaef0ce9b57b6570709f95a4bea4.JPG\n",
       "28014       Пантеры         6  d1fec8a6b6be63534c37f0a26e94c7e8.JPG\n",
       "\n",
       "[28015 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df \n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df.class_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_id</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3cf4207b958eade893a2f1618cf062b8.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>37698901280c871f426d40afe5c373cd.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>20e7b30026001cbfe0b5c0ee16c9ff56.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>a1bc8ea546206ee8fc0f1836fda9a5c1.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>54eb76914b84db8a0d56f98125abf588.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28010</th>\n",
       "      <td>5</td>\n",
       "      <td>07b420b4fe265b4ed918b46435c025d7.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28011</th>\n",
       "      <td>6</td>\n",
       "      <td>2d1c5918357bbdd729bf79085e55d35e.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28012</th>\n",
       "      <td>0</td>\n",
       "      <td>1531efa9f8687e390adf780355acd606.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28013</th>\n",
       "      <td>1</td>\n",
       "      <td>2b15eaef0ce9b57b6570709f95a4bea4.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28014</th>\n",
       "      <td>6</td>\n",
       "      <td>d1fec8a6b6be63534c37f0a26e94c7e8.JPG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28015 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class_id                            image_name\n",
       "0             5  3cf4207b958eade893a2f1618cf062b8.JPG\n",
       "1             2  37698901280c871f426d40afe5c373cd.JPG\n",
       "2             0  20e7b30026001cbfe0b5c0ee16c9ff56.JPG\n",
       "3             2  a1bc8ea546206ee8fc0f1836fda9a5c1.JPG\n",
       "4             5  54eb76914b84db8a0d56f98125abf588.JPG\n",
       "...         ...                                   ...\n",
       "28010         5  07b420b4fe265b4ed918b46435c025d7.JPG\n",
       "28011         6  2d1c5918357bbdd729bf79085e55d35e.JPG\n",
       "28012         0  1531efa9f8687e390adf780355acd606.JPG\n",
       "28013         1  2b15eaef0ce9b57b6570709f95a4bea4.JPG\n",
       "28014         6  d1fec8a6b6be63534c37f0a26e94c7e8.JPG\n",
       "\n",
       "[28015 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['unified_class'], inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image_name'] = 'train/'+df['image_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['image_name'].apply(lambda x: len(np.array(PIL.Image.open(x)).shape)==3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_id</th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>train/3cf4207b958eade893a2f1618cf062b8.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>train/37698901280c871f426d40afe5c373cd.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>train/20e7b30026001cbfe0b5c0ee16c9ff56.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>train/a1bc8ea546206ee8fc0f1836fda9a5c1.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>train/54eb76914b84db8a0d56f98125abf588.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28010</th>\n",
       "      <td>5</td>\n",
       "      <td>train/07b420b4fe265b4ed918b46435c025d7.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28011</th>\n",
       "      <td>6</td>\n",
       "      <td>train/2d1c5918357bbdd729bf79085e55d35e.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28012</th>\n",
       "      <td>0</td>\n",
       "      <td>train/1531efa9f8687e390adf780355acd606.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28013</th>\n",
       "      <td>1</td>\n",
       "      <td>train/2b15eaef0ce9b57b6570709f95a4bea4.JPG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28014</th>\n",
       "      <td>6</td>\n",
       "      <td>train/d1fec8a6b6be63534c37f0a26e94c7e8.JPG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28014 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       class_id                                  image_name\n",
       "0             5  train/3cf4207b958eade893a2f1618cf062b8.JPG\n",
       "1             2  train/37698901280c871f426d40afe5c373cd.JPG\n",
       "2             0  train/20e7b30026001cbfe0b5c0ee16c9ff56.JPG\n",
       "3             2  train/a1bc8ea546206ee8fc0f1836fda9a5c1.JPG\n",
       "4             5  train/54eb76914b84db8a0d56f98125abf588.JPG\n",
       "...         ...                                         ...\n",
       "28010         5  train/07b420b4fe265b4ed918b46435c025d7.JPG\n",
       "28011         6  train/2d1c5918357bbdd729bf79085e55d35e.JPG\n",
       "28012         0  train/1531efa9f8687e390adf780355acd606.JPG\n",
       "28013         1  train/2b15eaef0ce9b57b6570709f95a4bea4.JPG\n",
       "28014         6  train/d1fec8a6b6be63534c37f0a26e94c7e8.JPG\n",
       "\n",
       "[28014 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.cast_column(\"image_name\", Image())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80325d2c6997461eb816c30565d2995c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/28014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0b266e26dc4913bf20f8a2825671a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/28014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.class_encode_column(\"class_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['class_id', 'image_name'],\n",
       "    num_rows: 28014\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = \"microsoft/resnet-50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = dataset.train_test_split(test_size=0.1,stratify_by_column='class_id')\n",
    "train = splits['train']\n",
    "val = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(example_batch):\n",
    "    # Process each image individually and collect pixel values as tensors\n",
    "    di = [image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")['pixel_values'].squeeze(0) \n",
    "          for image in example_batch[\"image_name\"]]\n",
    "    # Stack the list of tensors into a single tensor with batch dimension\n",
    "    example_batch[\"pixel_values\"] = torch.stack(di)  \n",
    "    example_batch[\"labels\"] = torch.tensor(example_batch[\"class_id\"])\n",
    "\n",
    "    # Remove original columns no longer needed\n",
    "    del example_batch[\"image_name\"]\n",
    "    del example_batch[\"class_id\"]\n",
    "\n",
    "    return example_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_transform(load_image)\n",
    "val.set_transform(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[-1.4329, -1.4672, -1.5014,  ..., -1.7412, -1.8953, -1.8268],\n",
       "          [-1.4672, -1.4500, -1.4843,  ..., -1.4843, -1.6727, -1.7240],\n",
       "          [-1.5870, -1.5185, -1.5185,  ..., -0.6794, -0.9705, -1.3130],\n",
       "          ...,\n",
       "          [-1.8097, -1.7925, -1.7583,  ..., -1.6213, -1.6555, -1.6555],\n",
       "          [-1.7754, -1.7754, -1.7754,  ..., -1.6384, -1.6555, -1.6555],\n",
       "          [-1.7754, -1.7754, -1.7754,  ..., -1.6555, -1.6555, -1.6555]],\n",
       " \n",
       "         [[-1.4405, -1.4755, -1.5105,  ..., -1.7731, -1.9307, -1.8606],\n",
       "          [-1.4755, -1.4580, -1.4930,  ..., -1.4930, -1.6856, -1.7381],\n",
       "          [-1.5980, -1.5280, -1.5280,  ..., -0.6702, -0.9678, -1.3179],\n",
       "          ...,\n",
       "          [-1.7906, -1.7731, -1.7381,  ..., -1.6331, -1.6681, -1.6681],\n",
       "          [-1.7556, -1.7556, -1.7556,  ..., -1.6506, -1.6681, -1.6681],\n",
       "          [-1.7556, -1.7556, -1.7556,  ..., -1.6681, -1.6681, -1.6681]],\n",
       " \n",
       "         [[-1.2119, -1.2467, -1.2816,  ..., -1.5256, -1.6824, -1.6127],\n",
       "          [-1.2467, -1.2293, -1.2641,  ..., -1.2293, -1.4210, -1.4733],\n",
       "          [-1.3687, -1.2990, -1.2990,  ..., -0.4101, -0.7064, -1.0550],\n",
       "          ...,\n",
       "          [-1.5430, -1.5256, -1.4907,  ..., -1.4036, -1.4384, -1.4384],\n",
       "          [-1.5081, -1.5081, -1.5081,  ..., -1.4210, -1.4384, -1.4384],\n",
       "          [-1.5081, -1.5081, -1.5081,  ..., -1.4384, -1.4384, -1.4384]]]),\n",
       " 'labels': tensor(5)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels, average = 'macro' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([10]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([10, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    m,\n",
    "    num_labels=10,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=64,\n",
    "    #gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=5,\n",
    "    #warmup_ratio=0.1,\n",
    "    #logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    output_dir = 'save'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vadim\\AppData\\Local\\Temp\\ipykernel_18032\\2426107787.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args  =  args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96cb6ac6fa045b3992b09291b90759f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1755 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7a9eafb40b43a684b5fcec67dedd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25091850757598877, 'eval_f1': 0.8510007796379627, 'eval_runtime': 31.5472, 'eval_samples_per_second': 177.607, 'eval_steps_per_second': 2.789, 'epoch': 1.0}\n",
      "{'loss': 0.52, 'grad_norm': 0.7057911157608032, 'learning_rate': 0.00021452991452991453, 'epoch': 1.42}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659b94ec408e4fa991b67be733c1de40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1296033263206482, 'eval_f1': 0.922401637299774, 'eval_runtime': 31.0949, 'eval_samples_per_second': 180.19, 'eval_steps_per_second': 2.83, 'epoch': 2.0}\n",
      "{'loss': 0.106, 'grad_norm': 1.186615228652954, 'learning_rate': 0.00012905982905982903, 'epoch': 2.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60855795599d454fb7b69d1753626c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10487987846136093, 'eval_f1': 0.9446050525090781, 'eval_runtime': 31.575, 'eval_samples_per_second': 177.45, 'eval_steps_per_second': 2.787, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814cee3f524b48a08ec5f7c868ecf3a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10234136879444122, 'eval_f1': 0.9466697829533823, 'eval_runtime': 30.8799, 'eval_samples_per_second': 181.445, 'eval_steps_per_second': 2.85, 'epoch': 4.0}\n",
      "{'loss': 0.0382, 'grad_norm': 1.642487645149231, 'learning_rate': 4.358974358974359e-05, 'epoch': 4.27}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bc4bff2aa44c5abd8249b7a39bdc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10092779248952866, 'eval_f1': 0.9522334677747806, 'eval_runtime': 31.455, 'eval_samples_per_second': 178.127, 'eval_steps_per_second': 2.798, 'epoch': 5.0}\n",
      "{'train_runtime': 1138.7592, 'train_samples_per_second': 98.401, 'train_steps_per_second': 1.541, 'train_loss': 0.191856851604929, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(example_batch):\n",
    "    # Process each image individually and collect pixel values as tensors\n",
    "    di = [image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")['pixel_values'].squeeze(0) \n",
    "          for image in example_batch[\"image_name\"]]\n",
    "    # Stack the list of tensors into a single tensor with batch dimension\n",
    "    example_batch[\"pixel_values\"] = torch.stack(di)\n",
    "    \n",
    "    del example_batch[\"image_name\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
